# 如何选择机器学习算法-监督学习

监督学习算法的选择可以从4个层面考虑

1. 问题类型
2. 性能要求
3. 数据特性
4. 其他要求

## 问题类型

搜先需要明确的是什么类型的问题：分类or回归？

| Algorithm           | 回归or分类 |
| ------------------- | ------ |
| KNN                 | 都可以    |
| Linear regression   | 回归     |
| Logistic regression | 分类     |
| Naive Bayes         | 分类     |
| Decision trees      | 都可     |
| Random Forest       | 都可     |
| AdaBoost            | 都可     |
| Neural networks     | 都可     |
| SVM                 | 分类     |

分析

- 线性回归和逻辑回归本质上是一样的，只是因为逻辑回归使用了logic function使得目标函数对判别边界的点更敏感，因此更适合分类。

- tree-based是天然的分类模型，但当我们将“类”取得无线小时，就可以近似地作为回归模型使用

  ​



## 性能层面

性能要求：通常对于机器学习算法的最基本比较是性能。性能可以简单分为3种：准确度，训练性能，预测性能。我们很难抛开其他条件去比较算法的性能，因此我们只对平均性能进行比较。

| Algorithm           | 平均准确度  | 平均训练成本                              | 预测性能         |
| ------------------- | ------ | ----------------------------------- | ------------ |
| KNN                 | Lower  | Fast                                | Depends on n |
| Linear regression   | Lower  | Fast                                | Fast         |
| Logistic regression | Lower  | Fast                                | Fast         |
| Naive Bayes         | Lower  | Fast (excluding feature extraction) | Fast         |
| Decision trees      | Lower  | Fast                                | Fast         |
| Random Forest       | Higher | Slow                                | Moderate     |
| AdaBoost            | Higher | Slow                                | Fast         |
| Neural networks     | Higher | Slow                                | Fast         |

如上表我们可以发现

- 准确度和性能一般成反比
- 采用集成学习的Random Forest和Adaboost 一般会有更好的准确性
- 对于最终训练结果是公式的模型，其预测性能都是非常好的
  - KNN是基于统计的，不输出预测函数，预测时需要大量统计附近点的值，因此速度可能会比较慢
  - RF由于有“森林”存在，相当于使用n个树模型进行预测，所以也会慢
- 贝叶斯需要一个特征划分的过程，来降低特征数据的稀疏性，这个过程会相对复杂



## 数据特性

选择算法另一个需要考虑的重要原因是我们现有的数据是怎样的，如1

1. 样本量大小
2. 是否有很多无关特征
3. 是否能够判断变量之间存在的互相影响
4. 是否有特征scaling的必要性

| Algorithm         | 适合样本量 | 处理多余特征 | 反映特征之间的影响 | 是否需要feature scaling | 线性分类 |
| ----------------- | ----- | ------ | --------- | ------------------- | ---- |
| Knn               | 大     | 不能     | 不能        | 需要                  | 线性   |
| Linear Regression | 都可    | 不能     | 不能        | 不需要（除非正则）           | 线性   |
| Logic Regression  | 都可    | 不能     | 不能        | 不需要（除非正则）           | 线性   |
| Naive Bayes       | 都可    | 能      | 不能        | 不需要                 | 非    |
| Decision trees    | 大     | 不能     | 能         | 不需要                 | 非    |
| Random Forest     | 大     | 能      | 能         | 不需要                 | 非    |
| AdaBoost          | 大     | 能      | 能         | 不需要                 | 非    |
| Neural network    | 大     | 能      | 能         | 不需要                 | 非    |
| SVM               | 大     | 能      | 不能        | 不需要？                | 都可   |



## 其他要求

除了以上维度，还有一些需求是我们要考虑的：模型的解释性，调参难度

模型解释性上来看，KNN，LR，decision tree，贝叶斯的解释性都比较好，集成学习相对较差，神经网络则非常难以解释

调参难度，主要是看参数个数KNN，LR可调参数不多；贝叶斯在特征提取时需要调少量参数；tree-based模型有一些参数要调；神经网络要调的参数最多